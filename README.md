# ğŸš€ WarpMesh

<img width="100%" src="https://raw.githubusercontent.com/chunyang-w/chunyang-w.github.io/pic/202306231902481.webp"/>

(Image source: [Star Trek: Official Site](https://intl.startrek.com/article/warp-drive-a-reality-could-be-soon))

## ğŸ” About this project:

In the famous TV series Star Trek, the starship Enterprise is able to travel
faster than light by warping space-time. In this project, we 'warp' the
underlying mesh of a discretised PDE problem to win some computational time.
The node of the mesh is moved to the ideal position guided by a Graph Neural
Network (GNN), which is supposed to be faster than numerical solvers.


The latest test status:

[![WarpMesh](https://github.com/mesh-adaptation/UM2N/actions/workflows/test_warpmesh.yml/badge.svg)](https://github.com/mesh-adaptation/UM2N/actions/workflows/test_warpmesh.yml)


## ğŸ› ï¸ Installation

### The easy way:

Just navigate to **project root** folder, open terminal and execute the
`install/install.sh` shell script:

``` shell
./install.sh
```

This will install [Firedrake](https://www.firedrakeproject.org/download.html)
and [Movement](https://github.com/mesh-adaptation/movement) under the `install`
folder, as well as the `WarpMesh` package.


### The hard way (in case the easy way did not went well or you want to challenge yourself):

1. The mesh generation relies on Firedrake, which is a Python package. To
   install Firedrake, please follow the instructions on
   [firedrakeproject.org](https://www.firedrakeproject.org/download.html).

2. The movement of the mesh is implemented by `mesh-adaptation/movement`,
   install it in the Firedrake virtual environment. To install, run
   `pip install -e .` in the `mesh-adaptation/movement` directory. Here is a
   link to that repo: [mesh-adaptation/movement](https://github.com/mesh-adaptation/movement).

3. Use the virtual environment provided by Firedrake to install the dependencies
   of this project. The virtual environment is located at
   `~/firedrakevenv/bin/activate`. To activate the virtual environment, run
   `source ~/firedrakevenv/bin/activate`.

4. Run `pip install -e .` in the root directory of this project to install the
   package and its dependencies.


## ğŸ’¿ Dataset generation

In case you do not wish to generate the dataset by yourself, here is a
pre-generated dataset on Google Drive:
[link](https://drive.google.com/drive/folders/1sQ-9zWbTryCXwihqaqazrQ4Vp1MRdBPK?usp=sharing).
In this folder you can find all cases used to train/test the model. The naming
convention of the file is 'z=<0,1>_n_dist={number_of_distribution_used}_max_dist={maximum_distribution_used}_<{number_of_grid_in_x_direction}_{number_of_grid_in_y_direction}>_n={number_of_samples}_{data_set_type}'

If `n_dist = None`, then the number of Gaussian distribution used will be
randomly chosen from 1 to `max_dist`, otherwise, `n_dist` will be used to
generate a fixed number of Gaussian distribution version dataset.

The {data_set_type} will be either `'smpl'` or `'cmplx'`, indicating whether the
dataset is isotropic or anisotropic.

After download, you should put the downloaded folder `helmholtz` under
`data/dataset` folder.

### Generate the dataset by yourself

```{shell}
. script/make_dataset.sh
```
This command will make following datasets by solving Monge-AmpÃ¨re equation with
the following PDEs:

+ Burgers equation (on square domain)
+ Helmholtz equation (both square/random polygon domain)
+ Poisson equation (both square/random polygon domain)

User can modify the variables
```
n_dist_start=1
n_dist_end=10
n_grid_start=15
n_grid_end=35
```
defined in `script/make_dataset.sh` to generate datasets of different sizes.

The number of samples in the dataset can be changed by modifying the variable
`n_sample` in `script/build_helmholtz_dataset`.

## ğŸš€ Train the model

A training notebook is provided: `script/train_warpmesh.ipynb`. Further training
details can be found in the notebook.

Here is also a link to pre-trained models:
[link](https://drive.google.com/drive/folders/1P_JMpU1qmLdmbGTz8fL5VO-lEBoP3_2n?usp=sharing)

## ğŸ“Š Evaluate the model

There are a set of visualisation script under `script/` folder. The script can
be used to evaluate the model performance.

**Bear in mind that the path to datasets/model_weight in those files need
calibration**

## ğŸ“– Documentation
The documentation is generated by Sphinx. To build the documentation,under the
`docs` folder.


## ğŸ§© Project Layout

```
â”œâ”€â”€ warpmesh (Implementation of the project)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ generator (Dataset generator)
â”‚   â”œâ”€â”€ processor (Data processor)
â”‚   â”œâ”€â”€ helper (Helper functions)
â”‚   â”œâ”€â”€ loader (Customized dataset and dataloader)
â”‚   â”œâ”€â”€ model (MRN and M2N model implementation)
â”‚   â””â”€â”€ test (Simple tests for the model)
â”œâ”€â”€ data (Datasets are generated here)
â”‚   â”œâ”€â”€ dataset
â”‚   â””â”€â”€ output
â”œâ”€â”€ docs (Documentation)
â”‚   â”œâ”€â”€ conf.py
â”‚   â””â”€â”€ index.rst
â”œâ”€â”€ script (Utility scripts)
â”‚   â”œâ”€â”€ make_dataset.sh (Script for making datasets of different sizes)
â”‚   â”œâ”€â”€ build_helmholtz_dataset.py (Build helmholtz dataset)
â”‚   â”œâ”€â”€ compare.py (Compare the performance of different models)
â”‚   â”œâ”€â”€ evaluate.py 
â”‚   â”œâ”€â”€ gradual_change.py
â”‚   â”œâ”€â”€ plot.py
â”‚   â”œâ”€â”€ train_model.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tests
â”‚   â”œâ”€â”€ play_conv_feat.py
â”‚   â”œâ”€â”€ play_dataset.py
â”‚   â”œâ”€â”€ test_import.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ install.sh (Installation script for UM2N and its dependencies)
â”œâ”€â”€ pyproject.toml (Top-level metadata for Python project)
â””â”€â”€ README.md (Project summary and useful information)
```

## Useful thing: delete plot directory

```
find ./ -type d -name "plot" -exec rm -rf {} +

tensorboard --logdir=path/to/your/logs
```

## ğŸ––ğŸ¼ At last ...

Live long and prosper!  
